{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7984115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting open_clip_torch\n",
      "  Downloading open_clip_torch-2.32.0-py3-none-any.whl.metadata (31 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\tered\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.7.0)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.22.0-cp313-cp313-win_amd64.whl.metadata (6.3 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.7.0-cp313-cp313-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: regex in c:\\users\\tered\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from open_clip_torch) (2024.11.6)\n",
      "Collecting ftfy (from open_clip_torch)\n",
      "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tered\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from open_clip_torch) (4.67.1)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\tered\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from open_clip_torch) (0.32.3)\n",
      "Requirement already satisfied: safetensors in c:\\users\\tered\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from open_clip_torch) (0.5.3)\n",
      "Collecting timm (from open_clip_torch)\n",
      "  Downloading timm-1.0.15-py3-none-any.whl.metadata (52 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\tered\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\tered\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\tered\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\tered\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tered\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\tered\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (2025.5.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\tered\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\tered\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\tered\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torchvision) (11.2.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\tered\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\tered\\appdata\\roaming\\python\\python313\\site-packages (from ftfy->open_clip_torch) (0.2.13)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\tered\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub->open_clip_torch) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\tered\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub->open_clip_torch) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\tered\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub->open_clip_torch) (2.32.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\tered\\appdata\\roaming\\python\\python313\\site-packages (from tqdm->open_clip_torch) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tered\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tered\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->huggingface-hub->open_clip_torch) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tered\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->huggingface-hub->open_clip_torch) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tered\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->huggingface-hub->open_clip_torch) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tered\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->huggingface-hub->open_clip_torch) (2025.4.26)\n",
      "Downloading open_clip_torch-2.32.0-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.5/1.5 MB 737.4 kB/s eta 0:00:02\n",
      "   ------------- -------------------------- 0.5/1.5 MB 737.4 kB/s eta 0:00:02\n",
      "   -------------------- ------------------- 0.8/1.5 MB 711.7 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 1.0/1.5 MB 842.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 1.0 MB/s eta 0:00:00\n",
      "Downloading torchvision-0.22.0-cp313-cp313-win_amd64.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 0.5/1.7 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 0.8/1.7 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 0.8/1.7 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.3/1.7 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.3/1.7 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.6/1.7 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 1.1 MB/s eta 0:00:00\n",
      "Downloading torchaudio-2.7.0-cp313-cp313-win_amd64.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.5/2.5 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 0.8/2.5 MB 2.2 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.6/2.5 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.4/2.5 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.5/2.5 MB 2.6 MB/s eta 0:00:00\n",
      "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "Downloading timm-1.0.15-py3-none-any.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.8/2.4 MB 4.6 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.6/2.4 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.4/2.4 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 3.3 MB/s eta 0:00:00\n",
      "Installing collected packages: ftfy, torchvision, torchaudio, timm, open_clip_torch\n",
      "Successfully installed ftfy-6.3.1 open_clip_torch-2.32.0 timm-1.0.15 torchaudio-2.7.0 torchvision-0.22.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\tered\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\tered\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\tered\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tered\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: pillow in c:\\users\\tered\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (11.2.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\tered\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (2.2.6)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\tered\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\tered\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\tered\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\tered\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\tered\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\tered\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\tered\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\tered\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\tered\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (4.58.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\tered\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tered\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\tered\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\tered\\appdata\\roaming\\python\\python313\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tered\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install open_clip_torch torch torchvision torchaudio\n",
    "!pip install scikit-learn pandas matplotlib tqdm pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51f91ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tered\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import open_clip\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "829b19a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define paths\n",
    "covers_path = r\"C:\\Users\\tered\\Desktop\\recommender_system\\data\\images\\covers\"\n",
    "clean_csv_path = r\"C:\\Users\\tered\\Desktop\\recommender_system\\data\\processed\\books_covers_clean.csv\"\n",
    "experiment_output = r\"C:\\Users\\tered\\Desktop\\recommender_system\\experiments\\06_clip_kmeans\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d67e9e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Load the dataset\n",
    "df = pd.read_csv(clean_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "935273d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tered\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\tered\\.cache\\huggingface\\hub\\models--laion--CLIP-ViT-B-32-laion2B-s34B-b79K. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "    (patch_dropout): Identity()\n",
       "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): ModuleList(\n",
       "        (0-11): 12 x ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_1): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_2): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): ModuleList(\n",
       "      (0-11): 12 x ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 512)\n",
       "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: Load the CLIP model from open_clip_torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6fb5f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting CLIP embeddings using open_clip_torch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 1717/6481 [18:06<50:14,  1.58it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m image = preprocess(Image.open(image_file).convert(\u001b[33m\"\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m\"\u001b[39m)).unsqueeze(\u001b[32m0\u001b[39m).to(device)\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     embedding = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m.cpu().numpy()\n\u001b[32m     18\u001b[39m image_embeddings.append(embedding.squeeze())\n\u001b[32m     19\u001b[39m image_ids.append(book_id)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tered\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\open_clip\\model.py:279\u001b[39m, in \u001b[36mCLIP.encode_image\u001b[39m\u001b[34m(self, image, normalize)\u001b[39m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mencode_image\u001b[39m(\u001b[38;5;28mself\u001b[39m, image, normalize: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvisual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.normalize(features, dim=-\u001b[32m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m normalize \u001b[38;5;28;01melse\u001b[39;00m features\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tered\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tered\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tered\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\open_clip\\transformer.py:827\u001b[39m, in \u001b[36mVisionTransformer.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    825\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor):\n\u001b[32m    826\u001b[39m     x = \u001b[38;5;28mself\u001b[39m._embeds(x)\n\u001b[32m--> \u001b[39m\u001b[32m827\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    828\u001b[39m     pooled, tokens = \u001b[38;5;28mself\u001b[39m._pool(x)\n\u001b[32m    830\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.proj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tered\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tered\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tered\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\open_clip\\transformer.py:504\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, x, attn_mask)\u001b[39m\n\u001b[32m    502\u001b[39m         x = checkpoint(r, x, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, attn_mask, use_reentrant=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    503\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m504\u001b[39m         x = \u001b[43mr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.batch_first:\n\u001b[32m    507\u001b[39m     x = x.transpose(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)    \u001b[38;5;66;03m# LND -> NLD\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tered\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tered\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tered\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\open_clip\\transformer.py:267\u001b[39m, in \u001b[36mResidualAttentionBlock.forward\u001b[39m\u001b[34m(self, q_x, k_x, v_x, attn_mask)\u001b[39m\n\u001b[32m    265\u001b[39m v_x = \u001b[38;5;28mself\u001b[39m.ln_1_kv(v_x) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mln_1_kv\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m v_x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    266\u001b[39m x = q_x + \u001b[38;5;28mself\u001b[39m.ls_1(\u001b[38;5;28mself\u001b[39m.attention(q_x=\u001b[38;5;28mself\u001b[39m.ln_1(q_x), k_x=k_x, v_x=v_x, attn_mask=attn_mask))\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m x = x + \u001b[38;5;28mself\u001b[39m.ls_2(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mln_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tered\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tered\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tered\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\container.py:240\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tered\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tered\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tered\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Step 5: Extract image embeddings\n",
    "image_embeddings = []\n",
    "image_ids = []\n",
    "\n",
    "print(\"Extracting CLIP embeddings using open_clip_torch...\")\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    book_id = row['ID']\n",
    "    image_file = os.path.join(covers_path, f\"{book_id}.jpg\")\n",
    "\n",
    "    if not os.path.exists(image_file):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        image = preprocess(Image.open(image_file).convert(\"RGB\")).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            embedding = model.encode_image(image).cpu().numpy()\n",
    "        image_embeddings.append(embedding.squeeze())\n",
    "        image_ids.append(book_id)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {book_id}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f63d79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Convert embeddings to NumPy array\n",
    "embeddings_array = np.array(image_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088eaad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Apply KMeans clustering\n",
    "num_clusters = 10  # You can change this value\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(embeddings_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89db545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Save results into a DataFrame\n",
    "clip_df = pd.DataFrame(embeddings_array)\n",
    "clip_df['ID'] = image_ids\n",
    "clip_df['cluster'] = cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f019b3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "output_file = os.path.join(experiment_output, \"clip_embeddings_kmeans.csv\")\n",
    "clip_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"CLIP embeddings and clusters saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf61439",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PARTE 2\n",
    "\n",
    "# Step 1: Additional imports\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 2: Prepare for similarity and lookup\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Merge CLIP + original titles\n",
    "merged_df = pd.merge(df, clip_df, on='ID', how='inner')\n",
    "\n",
    "# Step 3: Get embeddings matrix\n",
    "embedding_cols = [col for col in clip_df.columns if isinstance(col, int) or col.isdigit()]\n",
    "features = clip_df[embedding_cols].values\n",
    "\n",
    "# Step 4: Calculate similarity matrix\n",
    "similarity_matrix = cosine_similarity(features)\n",
    "\n",
    "# Step 5: Define target book index\n",
    "target_index = list(merged_df.index[merged_df['ID'] == 2711])[0]\n",
    "target_title = merged_df.iloc[target_index]['title']\n",
    "target_id = merged_df.iloc[target_index]['ID']\n",
    "\n",
    "print(f\"Finding recommendations for book index: {target_index}\")\n",
    "print(f\"Selected book [{target_id}]: {target_title}\")\n",
    "\n",
    "# Step 6: Get top 10 most similar\n",
    "similarities = similarity_matrix[target_index]\n",
    "top_indices = np.argsort(similarities)[::-1][1:11]  # Exclude itself\n",
    "\n",
    "# Print recommendations\n",
    "print(\"Top 10 Recommendations based on CLIP + Cosine Similarity:\")\n",
    "for i, idx in enumerate(top_indices):\n",
    "    book_id = merged_df.iloc[idx]['ID']\n",
    "    title = merged_df.iloc[idx]['title']\n",
    "    score = similarities[idx]\n",
    "    print(f\"{i+1}. [{book_id}] {title} (Similarity: {score:.4f})\")\n",
    "\n",
    "# Step 7: Cosine Similarity Average\n",
    "avg_sim = np.mean(similarities[top_indices])\n",
    "print(f\"\\nAverage Similarity (Cosine) for top 10: {avg_sim:.4f}\")\n",
    "\n",
    "# Step 8: Silhouette Score\n",
    "sil_score = silhouette_score(features, clip_df['cluster'])\n",
    "print(f\"Silhouette Score (KMeans Clustering): {sil_score:.4f}\")\n",
    "\n",
    "# Step 9: Intra-list Diversity (average pairwise cosine distance)\n",
    "top_embeds = features[top_indices]\n",
    "pairwise_dist = pairwise_distances(top_embeds, metric='cosine')\n",
    "upper_triangle = pairwise_dist[np.triu_indices_from(pairwise_dist, k=1)]\n",
    "intra_div = np.mean(upper_triangle)\n",
    "print(f\"Intra-list Diversity: {intra_div:.4f}\")\n",
    "\n",
    "# Step 10: Visualize with t-SNE\n",
    "print(\"Generating t-SNE visualization...\")\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "tsne_results = tsne.fit_transform(features)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "scatter = plt.scatter(tsne_results[:,0], tsne_results[:,1], c=clip_df['cluster'], cmap='tab10', s=15)\n",
    "plt.title(\"t-SNE Visualization of Book Covers (CLIP Embeddings)\")\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.savefig(os.path.join(experiment_output, \"tsne_visualization.png\"))\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
